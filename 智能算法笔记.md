# 超表面优化算法总结

## 模拟退火算法(SA)

### 思想        

​		模拟退火是模拟物理上退火方法，通过N次退火，逼近系统**最值**的一种方法。模拟退火算法的思想借鉴于固体的退火原理，当固体的**温度很高**的时候，内能比较大，固体的内部粒子处于**快速无序运动**，当温度**慢慢降低**的过程中，固体的内能减小，粒子的慢慢**趋于有序**，最终，当固体处于常温时，**内能达到最小**，此时，粒子**最为稳定**。

### 算法流程

1. 我们先设定一个初始的温度$T$（这个温度会比较高，比如2000）
2. 每次循环都**退火一次**
3. 然后降低$T$的温度，我们通过让$T$和一个“降温系数”$\Delta T$（一个接近1的小数，比如$0.99$)相乘，达到**慢慢降低**温度的效果，直到接近于0（我们用$eps$来代表一个接近0的数(比如0.00001)，只要$T<eps$就可以退出循环了）

### 代码示例

```python
T = 2000 #代表开始的温度
dT = 0.99 #代表系数delta T
eps = 1e-14 #相当于0.0000000000000001

#用自变量计算函数值,这里可能存在多个自变量对应一个函数值的情况，比如f(x,y)
def func(x, ... ):
    //这里是对系统的结果进行计算
    ans = ......
    return ans

#原始值
x = rand() #x0取随机值
f = func(x,...) #通过自变量算出f(x0)的值
while(T > eps):
    #这里是每一次退火的操作
    #x1可以左右随机移动，幅度和温度T正相关，所以*T
    #注意这里移动可以左右移动，但是也可以单向移动
    dx = (2*rand() - RAND_MAX) * T
    #让x落在定义域内，如果没在里面，就重新随机。题目有要求需要写，否则不用写
    while(x > ? || x < ? ...):
        dx = (2*rand() - RAND_MAX) * T
    #求出f(x1)的值
    df = func(dx)
    #这里需要具体问题具体分析，我们要接受更加优秀的情况。可能是df < f(比如求最小值）
    if(f < df):
        f = df
        x = dx
        [...,y = dy] 
    #接受，替换值，如果多个自变量，那么都替换
    #否则概率接受，注意这里df-f也要具体问题具体分析。
    else if(exp((df - f) / T) * RAND_MAX > rand()):
        f = df
        x = dx 
       	[...y = dy] #接受，替换值，如果多个自变量，那么都替换
    T = T * dT #温度每次下降一点点， T * 0.99
#最后输出靠近最优的自变量x值，和函数值f(x)
print(x,f(x))
```

### 超表面优化算法流程

![SA](D:\UCAS\超表面\algorithms\SA.png)

### 缺点

SA算法的主要缺点是收敛缓慢。 SA算法容易陷入局部最小值，可能需要很长时间才能达到最优解。此外，初始温度和冷却时间表的选择可能会显着影响最终结果。因此，微调这些参数可能需要花费大量时间。最后，SA算法对输入数据中的噪声和异常值敏感。

## 遗传算法(GA)

### 思想

​		通过模拟自然选择过程的启发式算法。通过模拟生物DNA遗传过程，种群的交叉、变异、选择等仿生学操作实现系统的优化。

### 算法流程

1. 初始化种群：种群规模（用于种群之间的交叉）、初始DNA链（优化的变量）、繁衍代数（迭代次数）、变异概率、交叉概率。
2. 翻译DNA，将DNA转换成自己定义的变量
3. 计算适应度，就是计算变量所导致的结果，比如超表面的远场
4. 根据适应度大小有概率地淘汰，选出父辈，并进行交叉变异
5. 将经过交叉变异的种群进行下一轮迭代

### 代码示例

```python
DNA_SIZE = 20  # DNA size
CROSS_RATE = 0.1
MUTATE_RATE = 0.02
POP_SIZE = 500
N_GENERATIONS = 50

class GA(object):
    def __init__(self, DNA_size, cross_rate, mutation_rate, pop_size, ):
        self.DNA_size = DNA_size
        self.cross_rate = cross_rate
        self.mutate_rate = mutation_rate
        self.pop_size = pop_size

        self.pop = np.vstack([np.random.permutation(DNA_size) for _ in range(pop_size)])

    def translateDNA(self, DNA, city_position):     # get cities' coord in order
        line_x = np.empty_like(DNA, dtype=np.float64)
        line_y = np.empty_like(DNA, dtype=np.float64)
        for i, d in enumerate(DNA):
            city_coord = city_position[d]
            line_x[i, :] = city_coord[:, 0]
            line_y[i, :] = city_coord[:, 1]
        return line_x, line_y

    def get_fitness(self, line_x, line_y):
        total_distance = np.empty((line_x.shape[0],), dtype=np.float64)
        for i, (xs, ys) in enumerate(zip(line_x, line_y)):
            total_distance[i] = np.sum(np.sqrt(np.square(np.diff(xs)) + np.square(np.diff(ys))))
        fitness = np.exp(self.DNA_size * 2 / total_distance)
        return fitness, total_distance

    def select(self, fitness):
        idx = np.random.choice(np.arange(self.pop_size), size=self.pop_size, replace=True, p=fitness / fitness.sum())
        return self.pop[idx]

    def crossover(self, parent, pop):
        if np.random.rand() < self.cross_rate:
            i_ = np.random.randint(0, self.pop_size, size=1)                        # select another individual from pop
            cross_points = np.random.randint(0, 2, self.DNA_size).astype(bool)   # choose crossover points
            keep_city = parent[~cross_points]                                       # find the city number
            swap_city = pop[i_, np.isin(pop[i_].ravel(), keep_city, invert=True)]
            parent[:] = np.concatenate((keep_city, swap_city))
        return parent

    def mutate(self, child):
        for point in range(self.DNA_size):
            if np.random.rand() < self.mutate_rate:
                swap_point = np.random.randint(0, self.DNA_size)
                swapA, swapB = child[point], child[swap_point]
                child[point], child[swap_point] = swapB, swapA
        return child

    def evolve(self, fitness):
        pop = self.select(fitness)
        pop_copy = pop.copy()
        for parent in pop:  # for every parent
            child = self.crossover(parent, pop_copy)
            child = self.mutate(child)
            parent[:] = child
        self.pop = pop
        
        
ga=GA(DNA_size=N_CITIES,cross_rate=CROSS_RATE,mutation_rate=MUTATE_RATE,pop_size=POP_SIZE)
for generation in range(N_GENERATIONS):
    ... = ga.translateDNA(ga.pop, ...)
    fitness,total_distance = ga.get_fitness(...)
    ga.evolve(fitness)
    best_idx = np.argmax(fitness)
```

### 超表面优化算法流程

1. 生成多个超表面变量作为初始种群
2. 计算传播远场，与理想远场进行比较，评价得到适应值
3. 根据适应值有概率的淘汰低适应度的超表面，选择出父辈超表面
4. 交叉变异产生新一代种群开始反复迭代

### 缺点

有可能种群的向着不好的方向改变从而丢掉好的DNA

## 微生物遗传算法(MGA)

### 思想

​		在遗传算法的基础上，加入了微生物遗传的概念。 遗传算法有个问题就是如何有效保留优秀的父辈(Elitism)，让好的父辈基因不会消失掉。这也是永远都给自己留条后路的意思。 MGA就是一个很好的保留 Elitism 的算法。**一句话来概括：在袋子里抽两个球，对比两个球，把球大的放回袋子里, 把球小的变一下再放回袋子里**，这样在这次选择中，大球不会被改变任何东西，就被放回了袋子，当作下一代的一部分。

### 算法流程

![MGA](D:\UCAS\超表面\algorithms\MGA.png)

​		我们有一个种群 `population`， 每次在进化的时候，我们会从这个 `pop` 中随机抽 2 个 DNA 出来, 然后对比一下他们的适应度`fitness`，我们将 `fitness` 高的定义成 `winner`，反之是 `loser`。我们不会去动任何 `winner` 的 DNA，要动手脚的只有这个 `loser`，比如对 `loser` 进行 `crossover` 和 `mutate`。动完手脚后将 `winner` 和 `loser` 一同放回 `pop` 中。

### 代码示例

从代码上看，比之前的GA少了个select环节，因为在进化环节中会进行select，所以把选择放在了evolve中。

```python
class MGA(object):
    def __init__(self, DNA_size, DNA_bound, cross_rate, mutation_rate, pop_size):
        self.DNA_size = DNA_size
        DNA_bound[1] += 1
        self.DNA_bound = DNA_bound
        self.cross_rate = cross_rate
        self.mutate_rate = mutation_rate
        self.pop_size = pop_size

        # initial DNAs for winner and loser
        self.pop = np.random.randint(*DNA_bound, size=(1, self.DNA_size)).repeat(pop_size, axis=0)

    def translateDNA(self, pop):
        # convert binary DNA to decimal and normalize it to a range(0, 5)
        return pop.dot(2 ** np.arange(self.DNA_size)[::-1]) / float(2 ** self.DNA_size - 1) * X_BOUND[1]

    def get_fitness(self, product):
        return product      # it is OK to use product value as fitness in here

    def crossover(self, loser_winner):      # crossover for loser
        cross_idx = np.empty((self.DNA_size,)).astype(np.bool)
        for i in range(self.DNA_size):
            cross_idx[i] = True if np.random.rand() < self.cross_rate else False  # crossover index
        loser_winner[0, cross_idx] = loser_winner[1, cross_idx]  # assign winners genes to loser
        return loser_winner

    def mutate(self, loser_winner):         # mutation for loser
        mutation_idx = np.empty((self.DNA_size,)).astype(np.bool)
        for i in range(self.DNA_size):
            mutation_idx[i] = True if np.random.rand() < self.mutate_rate else False  # mutation index
        # flip values in mutation points
        loser_winner[0, mutation_idx] = ~loser_winner[0, mutation_idx].astype(np.bool)
        return loser_winner

    def evolve(self, n):    # nature selection wrt pop's fitness
        for _ in range(n):  # random pick and compare n times
            sub_pop_idx = np.random.choice(np.arange(0, self.pop_size), size=2, replace=False)
            sub_pop = self.pop[sub_pop_idx]             # pick 2 from pop
            product = F(self.translateDNA(sub_pop))
            fitness = self.get_fitness(product)
            loser_winner_idx = np.argsort(fitness)
            loser_winner = sub_pop[loser_winner_idx]    # the first is loser and second is winner
            loser_winner = self.crossover(loser_winner)
            loser_winner = self.mutate(loser_winner)
            self.pop[sub_pop_idx] = loser_winner

        DNA_prod = self.translateDNA(self.pop)
        pred = F(DNA_prod)
        return DNA_prod, pred
```

### 超表面优化算法

1. 生成多个超表面变量作为初始种群
2. 计算传播远场，与理想远场进行比较，评价得到适应值
3. 根据适应度选择出一对作为父辈的超表面
4. 对比适应度，选出胜者和败者
5. 对败者进行交叉变异操作



## 进化策略

### 思想

​		在GA的基础上加入变异强度的概念，同时讲原来二进制的DNA转换成相应的实数数组。在一个种群中杀死结果差的，保留结果好的，让好的结果做为父代交叉变异出新的子代，子代竞争。

### 算法流程

1. 初始化种群产生第一批父代
2. 父代DNA以及变异强度之间交叉产生子代
3. 根据变异强度对子代进行变异
4. 对子代的变异强度也进行变异，这时候变异强度也能收敛
5. 对子代按照适应度进行排序
6. 选出前面的子代丢弃后面的子代

### 代码示例

```python
"""
The Evolution Strategy can be summarized as the following term:
{mu/rho +, lambda}-ES
Here we use following term to find a maximum point.
{n_pop/n_pop + n_kid}-ES
"""
import numpy as np
import matplotlib.pyplot as plt

DNA_SIZE = 1             # DNA (real number)
DNA_BOUND = [0, 5]       # solution upper and lower bounds
N_GENERATIONS = 200
POP_SIZE = 100           # population size
N_KID = 50               # n kids per generation


def F(x): return np.sin(10*x)*x + np.cos(2*x)*x     # to find the maximum of this function


# find non-zero fitness for selection
def get_fitness(pred): return pred.flatten()


def make_kid(pop, n_kid):
    # generate empty kid holder
    kids = {'DNA': np.empty((n_kid, DNA_SIZE))}
    kids['mut_strength'] = np.empty_like(kids['DNA'])
    for kv, ks in zip(kids['DNA'], kids['mut_strength']):
        # crossover (roughly half p1 and half p2)
        p1, p2 = np.random.choice(np.arange(POP_SIZE), size=2, replace=False)
        cp = np.random.randint(0, 2, DNA_SIZE, dtype=np.bool)  # crossover points
        kv[cp] = pop['DNA'][p1, cp]
        kv[~cp] = pop['DNA'][p2, ~cp]
        ks[cp] = pop['mut_strength'][p1, cp]
        ks[~cp] = pop['mut_strength'][p2, ~cp]

        # mutate (change DNA based on normal distribution)
        ks[:] = np.maximum(ks + (np.random.rand(*ks.shape)-0.5), 0.)    # must > 0
        kv += ks * np.random.randn(*kv.shape)
        kv[:] = np.clip(kv, *DNA_BOUND)    # clip the mutated value
    return kids


def kill_bad(pop, kids):
    # put pop and kids together
    for key in ['DNA', 'mut_strength']:
        pop[key] = np.vstack((pop[key], kids[key]))

    fitness = get_fitness(F(pop['DNA']))            # calculate global fitness
    idx = np.arange(pop['DNA'].shape[0])
    good_idx = idx[fitness.argsort()][-POP_SIZE:]   # selected by fitness ranking (not value)
    for key in ['DNA', 'mut_strength']:
        pop[key] = pop[key][good_idx]
    return pop


pop = dict(DNA=5 * np.random.rand(1, DNA_SIZE).repeat(POP_SIZE, axis=0),   # initialize the pop DNA values
           mut_strength=np.random.rand(POP_SIZE, DNA_SIZE))                # initialize the pop mutation strength values

plt.ion()       # something about plotting
x = np.linspace(*DNA_BOUND, 200)
plt.plot(x, F(x))

for _ in range(N_GENERATIONS):
    # something about plotting
    if 'sca' in globals(): sca.remove()
    sca = plt.scatter(pop['DNA'], F(pop['DNA']), s=200, lw=0, c='red', alpha=0.5); plt.pause(0.05)

    # ES part
    kids = make_kid(pop, N_KID)
    pop = kill_bad(pop, kids)   # keep some good parent for elitism

plt.ioff(); plt.show()
```

## 1+1进化策略

### 思想

一个父亲和孩子的竞争。

核心改进：放弃了交叉环节只有变异，全局变异强度的优化，1/5原则。

### 算法流程

1. 初始化种群，这时候只有一个父辈
2. 变异产生一个子代
3. 子代与父亲按照适应度竞争，如果子代更好就替代掉父辈
4. 根据子代变异的结果，如果1/5是比父代好的，就说明整个种群在收敛，那么变异强度减小（$\sigma = \sigma * \exp({\frac{1}{3}})$）,反之变异强度增大（$\sigma = \sigma * \exp({-\frac{1}{12}})$）

### 代码示例

```python
import numpy as np
import matplotlib.pyplot as plt

DNA_SIZE = 1             # DNA (real number)
DNA_BOUND = [0, 5]       # solution upper and lower bounds
N_GENERATIONS = 200
MUT_STRENGTH = 5.        # initial step size (dynamic mutation strength)

def F(x): return np.sin(10*x)*x + np.cos(2*x)*x     # to find the maximum of this function

# find non-zero fitness for selection
def get_fitness(pred): return pred.flatten()

def make_kid(parent):
    # no crossover, only mutation
    k = parent + MUT_STRENGTH * np.random.randn(DNA_SIZE)
    k = np.clip(k, *DNA_BOUND)
    return k

def kill_bad(parent, kid):
    global MUT_STRENGTH
    fp = get_fitness(F(parent))[0]
    fk = get_fitness(F(kid))[0]
    p_target = 1/5
    if fp < fk:     # kid better than parent
        parent = kid
        ps = 1.     # kid win -> ps = 1 (successful offspring)
    else:
        ps = 0.
    # adjust global mutation strength
    MUT_STRENGTH *= np.exp(1/np.sqrt(DNA_SIZE+1) * (ps - p_target)/(1 - p_target))
    return parent

parent = 5 * np.random.rand(DNA_SIZE)   # parent DNA

plt.ion()
x = np.linspace(*DNA_BOUND, 200)

for _ in range(N_GENERATIONS):
    # ES part
    kid = make_kid(parent)
    py, ky = F(parent), F(kid)       # for later plot
    parent = kill_bad(parent, kid)

# something about plotting
plt.cla()
plt.scatter(parent, py, s=200, lw=0, c='red', alpha=0.5,)
plt.scatter(kid, ky, s=200, lw=0, c='blue', alpha=0.5)
plt.text(0, -7, 'Mutation strength=%.2f' % MUT_STRENGTH)
plt.plot(x, F(x)); plt.pause(0.05)

plt.ioff(); plt.show()
```

## 神经进化策略

### 思想

用梯度指导进化的方向，向梯度减少的方向进化。



